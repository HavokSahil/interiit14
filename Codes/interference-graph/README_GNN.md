# GNN Interference Predictor

This system uses a **Graph Neural Network (GNN)** to predict interference relationships between Access Points (APs) based on their operational state and client roaming patterns.

## 1. Problem Statement
Predict the **interference weight** (0.0 to 1.0) between any pair of APs without relying on direct physical measurements (like distance or path loss), using only logical network metrics.

## 2. Dataset & Features

### Node Features (Input)
Each AP is represented as a node with **6 features**:
1. **Incoming Energy (dBm)**: Total signal energy received from other sources.
2. **Allocated Throughput (Mbps)**: Total bandwidth currently used by clients.
3. **Connected Clients**: Number of currently associated devices.
4. **Duty Cycle**: Fraction of time the AP is transmitting (0-1).
5. **Roaming In**: Count of clients that roamed *to* this AP recently.
6. **Roaming Out**: Count of clients that roamed *from* this AP recently.

### Edge Labels (Target)
- **Target**: Continuous interference weight (0.0 - 1.0).
- **Source**: Derived from simulation physics (RSSI, channel overlap, traffic load).
- **Interpretation**:
  - `0.0`: No interference
  - `1.0`: Maximum interference

### Data Generation
Data is generated by simulating diverse network scenarios:
- **Topologies**: Grid, Random, Clustered, Linear.
- **Variations**: 3-7 APs, 10-50 clients, random channels/power.
- **Output**: Time-series logs converted into graph snapshots.

## 3. Model Architecture

The model uses a **Graph Attention Network (GAT)** backbone with an edge regression head.

### Architecture Components
1. **GAT Encoder**:
   - **Layer 1**: 6 inputs $\to$ 32 hidden units $\times$ 4 heads.
   - **Layer 2**: 128 inputs $\to$ 32 hidden units (ELU activation).
   - **Role**: Learns AP embeddings by aggregating information from "neighboring" APs (attention mechanism).

2. **Edge Predictor (MLP)**:
   - Takes embeddings of two nodes: $(h_u, h_v)$.
   - Concatenates them: $[h_u || h_v]$.
   - **MLP**: Linear $\to$ ReLU $\to$ Linear $\to$ ReLU $\to$ Linear.
   - **Output**: Single scalar clamped to $[0, 1]$.

### Loss Function
- **MSE Loss** (Mean Squared Error): Minimizes the squared difference between predicted and actual interference weights.

## 4. Training Pipeline

- **Optimizer**: Adam ($lr=0.001$)
- **Scheduler**: ReduceLROnPlateau (decays learning rate when loss plateaus)
- **Early Stopping**: Stops if validation loss doesn't improve for 20 epochs.
- **Metrics**:
  - **MSE**: Mean Squared Error
  - **MAE**: Mean Absolute Error
  - **Detection Accuracy**: Accuracy of detecting edges with weight > 0.1.

## 5. Usage

### Step 1: Generate Data
Create a diverse dataset (e.g., 50 scenarios):
```bash
python generate_training_data.py --scenarios 50 --steps 100
```

### Step 2: Train Model
Train the GNN on the generated logs:
```bash
python train_gnn.py
```
*Saves best model to `models/best_model.pt`*

### Step 3: Evaluate
Check performance on held-out test data:
```bash
python evaluate_gnn.py
```

### Step 4: Live Visualization
Run the simulation and press **'P'** to see live predictions:
```bash
python main.py
```
- **Red/Yellow Edges**: Actual interference (Ground Truth).
- **Blue Edges**: GNN Predicted interference.
