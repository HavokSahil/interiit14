\documentclass[11pt, a4paper]{report}
\usepackage[left=3cm,top=3cm,bottom=3cm,right=3cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
\geometry{margin=1in}


\lstset{
    basicstyle=\footnotesize\ttfamily,
    breaklines=true
}

\usepackage{titlesec}

\titlespacing*{\chapter}{0pt}{0pt}{1em}

\titleformat{\chapter}
  {\normalfont\Huge\bfseries}
  {\thechapter.}{1em}{}

 \usepackage[colorlinks=true,
            linkcolor=blue,
            urlcolor=blue,
            citecolor=blue]{hyperref}

\title{Edge-Aware Graph Neural Networks for Interference Graph Prediction in Wireless Access Networks}
\author{GNN Division\\ \textit{Team 15}}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
We present an EdgeConv-based Graph Neural Network architecture for predicting pairwise interference weights between Access Points in wireless networks. The model achieves R\textsuperscript{2}=0.954 and Pearson correlation r=0.981 on held-out test data, with mean absolute error MAE=0.0253 across 160 test edges. Our approach demonstrates that graph-structured neural architectures can efficiently approximate computationally expensive interference simulations while maintaining prediction accuracy suitable for network planning applications.
\end{abstract}

\tableofcontents

\chapter{Introduction}

\section{Problem Formulation}
Given a wireless network with $N$ Access Points (APs), we model interference relationships as a weighted graph $G = (V, E, W)$ where:
\begin{itemize}
    \item $V = \{v_1, \ldots, v_N\}$ represents APs with feature vectors $\mathbf{x}_i \in \mathbb{R}^d$
    \item $E \subseteq V \times V$ represents potential interference pairs
    \item $W: E \rightarrow [0,1]$ assigns interference weights to edges
\end{itemize}

The objective is to learn a function $f_\theta: \mathcal{G} \rightarrow \mathbb{R}^{|E|}$ that predicts edge weights $\hat{w}_{ij} = f_\theta(G)_{ij}$ from graph structure and node features, minimizing:
\[
\mathcal{L} = \frac{1}{|E|} \sum_{(i,j) \in E} \ell(\hat{w}_{ij}, w_{ij})
\]

\section{Motivation}
Traditional electromagnetic interference simulation requires $O(N^2M)$ computations for $N$ APs and $M$ clients. Our learned approach reduces inference to a single forward pass with complexity $O(|E|d)$, enabling real-time network optimization.

\section{Contributions}
\begin{enumerate}
    \item Formulation of wireless interference prediction as a graph edge regression problem
    \item Implementation of EdgeConv-based GNN achieving R\textsuperscript{2}=0.954 on test data
    \item Demonstration of 200-500$\times$ computational speedup over traditional simulation
    \item Comprehensive error analysis across interference strength ranges
\end{enumerate}

\chapter{Dataset Construction}

\section{Simulation Environment}
Network snapshots were generated using a discrete-event simulator capturing AP-client associations, roaming events, and throughput metrics. The simulator implements:
\begin{itemize}
    \item IEEE 802.11 channel model with path loss exponent $\alpha = 2.5$
    \item Dynamic client mobility with Poisson-distributed roaming events ($\lambda = 0.1$ events/sec)
    \item Proportional fair scheduling for client-AP associations
\end{itemize}

\section{Graph Representation}

\subsection{Node Features}
Each snapshot $t$ produces a graph $G_t$ with node features ($d=9$):
\begin{align*}
\mathbf{x}_i &= [E_i, T_i, C_i, D_i, R^{\text{in}}_i, R^{\text{out}}_i, \text{ch}_i, \text{bw}_i, P_i]^\top
\end{align*}
where:
\begin{itemize}
    \item $E_i$: Energy consumption (dBm)
    \item $T_i$: Throughput (Mbps)
    \item $C_i$: Number of associated clients
    \item $D_i$: Duty cycle $\in [0,1]$
    \item $R^{\text{in}}_i, R^{\text{out}}_i$: Incoming/outgoing roaming events
    \item $\text{ch}_i$: Channel number
    \item $\text{bw}_i$: Bandwidth (MHz)
    \item $P_i$: Transmit power (dBm)
\end{itemize}

\subsection{Edge Weights}
Interference strength $w_{ij} \in [0,1]$ computed from overlapping coverage areas and shared client contention.

\subsection{Feature Normalization}
Features standardized via $\tilde{x}_i = (x_i - \mu) / \sigma$ with:
\begin{align*}
\boldsymbol{\mu} &= [-49.15, 80.56, 5.0, 0.537, 0, 0, 3, 20, 25]^\top \\
\boldsymbol{\sigma} &= [6.33, 49.42, 3.18, 0.33, 1, 1, 4, 1, 1]^\top
\end{align*}

\section{Dataset Statistics}
\begin{table}[h!]
\centering
\begin{tabular}{lc}
\toprule
Property & Value \\
\midrule
Total snapshots & 53 \\
Train/Val/Test split & 37/8/8 \\
Average $|V|$ per graph & 5.0 \\
Average $|E|$ per graph & 20.0 \\
$w_{ij}$ distribution & $\min=0.000$, $\max=0.539$, $\mu=0.237$, $\sigma=0.194$ \\
Total edges & 1060 \\
AP states logged & 265 \\
Client states logged & 1325 \\
Roaming events & 67 \\
\bottomrule
\end{tabular}
\caption{Dataset characteristics}
\end{table}

\chapter{Model Architecture}

\section{EdgeConv Layer}
The EdgeConv operation for node $i$ aggregates information from neighbors $\mathcal{N}(i)$:
\[
\mathbf{h}_i^{(\ell+1)} = \max_{j \in \mathcal{N}(i)} \text{MLP}^{(\ell)}\left([\mathbf{h}_i^{(\ell)} \| \mathbf{h}_j^{(\ell)} - \mathbf{h}_i^{(\ell)}]\right)
\]
where $\|$ denotes concatenation and MLP is a multi-layer perceptron. This captures relative feature differences crucial for modeling interference.

\section{Network Architecture}
\begin{algorithm}[h!]
\caption{EdgeConv GNN Forward Pass}
\begin{algorithmic}
\STATE \textbf{Input:} Graph $G$, node features $\mathbf{X} \in \mathbb{R}^{N \times 9}$
\STATE $\mathbf{H}^{(0)} \leftarrow \mathbf{X}$
\FOR{$\ell = 1$ to $3$}
    \STATE $\mathbf{H}^{(\ell)} \leftarrow \text{EdgeConv}(\mathbf{H}^{(\ell-1)}, E)$
    \STATE $\mathbf{H}^{(\ell)} \leftarrow \text{BatchNorm}(\mathbf{H}^{(\ell)})$
    \STATE $\mathbf{H}^{(\ell)} \leftarrow \text{ReLU}(\mathbf{H}^{(\ell)})$
    \STATE $\mathbf{H}^{(\ell)} \leftarrow \text{Dropout}(\mathbf{H}^{(\ell)}, p=0.2)$
\ENDFOR
\STATE $\hat{\mathbf{W}} \leftarrow \text{EdgePredictor}(\mathbf{H}^{(3)}, E)$
\STATE \textbf{Output:} Predicted edge weights $\hat{\mathbf{W}}$
\end{algorithmic}
\end{algorithm}

\section{Configuration Parameters}
\begin{table}[h!]
\centering
\begin{tabular}{lc}
\toprule
Parameter & Value \\
\midrule
Input dimension & $d_{\text{in}} = 9$ \\
Hidden dimension & $d_h = 32$ \\
Number of EdgeConv layers & $L = 3$ \\
Dropout probability & $p = 0.2$ \\
Activation function & ReLU \\
Normalization & Batch Normalization \\
Total parameters & 14,401 \\
\bottomrule
\end{tabular}
\caption{Model architecture parameters}
\end{table}

\chapter{Training Methodology}

\section{Loss Function}
Weighted mean squared error with emphasis on positive interference:
\[
\mathcal{L} = \frac{1}{|E|} \sum_{(i,j) \in E} \alpha_{ij} (\hat{w}_{ij} - w_{ij})^2
\]
where $\alpha_{ij} = 3.0$ for $w_{ij} > 0$, else $\alpha_{ij} = 1.0$.

\section{Optimization Strategy}

\subsection{Optimizer Configuration}
\begin{itemize}
    \item Algorithm: Adam optimizer
    \item Parameters: $\beta_1=0.9, \beta_2=0.999$
    \item Learning rate: $\eta_0 = 10^{-3}$
    \item Weight decay: $\lambda = 10^{-5}$
\end{itemize}

\subsection{Learning Rate Scheduling}
ReduceLROnPlateau scheduler with:
\begin{itemize}
    \item Reduction factor: 0.5
    \item Patience: 10 epochs
    \item Monitored metric: Validation loss
\end{itemize}

\subsection{Regularization Techniques}
\begin{itemize}
    \item Gradient clipping: $\|\nabla\|_2 \leq 1.0$
    \item Dropout: $p = 0.2$
    \item Early stopping: patience=30 epochs on validation loss
\end{itemize}

\section{Training Configuration}
\begin{table}[h!]
\centering
\begin{tabular}{lc}
\toprule
Parameter & Value \\
\midrule
Maximum epochs & 150 \\
Actual epochs (early stop) & 45 \\
Batch processing & Full graphs \\
Hardware & CPU (Intel-based) \\
Training time & $\approx$ 12 minutes \\
\bottomrule
\end{tabular}
\caption{Training configuration and computational resources}
\end{table}

\chapter{Experimental Results}

\section{Training Dynamics}
\begin{table}[h!]
\centering
\small
\begin{tabular}{ccccccc}
\toprule
Epoch & $\mathcal{L}_{\text{train}}$ & $\mathcal{L}_{\text{val}}$ & MSE & MAE & RMSE & R\textsuperscript{2} \\
\midrule
1  & 0.0695 & 0.0160 & 0.0160 & 0.1030 & 0.1264 & 0.576 \\
5  & 0.0020 & 0.0016 & 0.0020 & 0.0336 & 0.0452 & 0.946 \\
10 & 0.0014 & 0.0013 & 0.0015 & 0.0249 & 0.0390 & 0.960 \\
15 & 0.0014 & 0.0012 & 0.0012 & 0.0236 & 0.0347 & 0.968 \\
20 & 0.0013 & 0.0014 & 0.0014 & 0.0246 & 0.0373 & 0.963 \\
40 & 0.0006 & 0.0013 & 0.0013 & 0.0231 & 0.0367 & 0.964 \\
45* & 0.0005 & 0.0013 & 0.0013 & 0.0228 & 0.0364 & 0.965 \\
\bottomrule
\end{tabular}
\caption{Training progression. *Early stopping triggered.}
\end{table}

\section{Test Set Performance}
Evaluation on 8 held-out graphs (160 edges):

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
Metric & Value & Improvement & Baseline \\
\midrule
MSE & $1.7 \times 10^{-3}$ & -- & -- \\
MAE & 0.0253 & $-75.6\%$ & 0.104 \\
RMSE & 0.0407 & $-67.8\%$ & 0.127 \\
R\textsuperscript{2} & 0.954 & -- & 0.000 \\
Pearson $r$ & 0.981 & -- & -- \\
\midrule
\multicolumn{4}{l}{\textit{Prediction statistics}} \\
$\mu_{\hat{w}}$ & 0.2395 & +5.5\% & $\mu_w = 0.2270$ \\
$\sigma_{\hat{w}}$ & 0.1973 & +3.8\% & $\sigma_w = 0.1900$ \\
\bottomrule
\end{tabular}
\caption{Test set metrics compared to mean predictor baseline}
\end{table}

\section{Stratified Error Analysis}
Performance across interference strength ranges:

\begin{table}[h!]
\centering
\begin{tabular}{lccc}
\toprule
Weight Range & MAE & RMSE & Sample Count \\
\midrule
Low ($w < 0.2$) & 0.0041 & 0.0052 & 64 \\
Medium ($0.2 \leq w < 0.5$) & 0.0402 & 0.0498 & 93 \\
High ($w \geq 0.5$) & 0.0150 & 0.0163 & 3 \\
\bottomrule
\end{tabular}
\caption{Error stratification by interference strength}
\end{table}

\section{Visual Analysis}
\begin{figure}[h!]
\centering
\includegraphics[width=0.95\textwidth]{../models/training_curves.png}
\caption{Training dynamics: (a) Loss convergence, (b) RMSE reduction, (c) R\textsuperscript{2} improvement, (d) Correlation increase, (e) Learning rate schedule}
\end{figure}

\chapter{Analysis and Discussion}

\section{Model Capacity Analysis}
With 14,401 parameters predicting 20 edges per graph on average, the model achieves a parameter-to-prediction ratio of 720:1. This suggests efficient feature representation without overfitting, as evidenced by convergence of training and validation losses.

\section{Computational Efficiency}
\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\toprule
Method & Time per Graph & Speedup \\
\midrule
Traditional simulation & 2-5 seconds & 1$\times$ \\
GNN inference (CPU) & $\sim$10 ms & 200-500$\times$ \\
\bottomrule
\end{tabular}
\caption{Computational efficiency comparison}
\end{table}

\section{Error Characteristics}
The model exhibits three key properties:

\begin{enumerate}
    \item \textbf{Low bias}: Predicted mean within 5.5\% of true mean
    \item \textbf{Calibrated uncertainty}: Predicted standard deviation within 3.8\% of true standard deviation
    \item \textbf{Range-dependent accuracy}: 
    \begin{itemize}
        \item Exceptional performance on low-weight edges (MAE=0.0041)
        \item Moderate accuracy on medium-weight edges (MAE=0.0402)
        \item Limited samples for high-weight edges (only 3 instances)
    \end{itemize}
\end{enumerate}

\section{Strengths}
\begin{itemize}
    \item High correlation (r=0.981) indicates strong linear relationship between predictions and ground truth
    \item Low MAE on sparse interference patterns critical for dense deployments
    \item Lightweight architecture enables edge device deployment
    \item Fast inference suitable for real-time applications
\end{itemize}

\section{Limitations}
\begin{itemize}
    \item Small dataset (53 snapshots) limits assessment of generalization to diverse network topologies
    \item Fully connected graph assumption may not scale efficiently beyond $N \approx 50$ APs
    \item Static snapshot approach ignores temporal dynamics of client mobility
    \item Absence of uncertainty quantification prevents confidence-aware predictions
    \item Limited high-weight interference samples hinder robust learning in high-contention scenarios
\end{itemize}

\chapter{Conclusion}

\section{Summary}
This work demonstrates that EdgeConv-based Graph Neural Networks can accurately predict wireless interference graphs from Access Point feature vectors. The model achieves test R\textsuperscript{2}=0.954 with Pearson correlation r=0.981, representing a 200-500$\times$ speedup over traditional simulation while maintaining prediction accuracy suitable for network planning applications.

\section{Key Findings}
\begin{itemize}
    \item Edge-aware message passing effectively captures pairwise interference relationships
    \item Lightweight architecture (14,401 parameters) prevents overfitting on small datasets
    \item Prediction accuracy is highest for low-interference edges, critical for dense network deployments
    \item Real-time inference enables dynamic network optimization workflows
\end{itemize}

\section{Impact}
The proposed approach enables network operators to rapidly evaluate interference characteristics during planning phases, facilitating:
\begin{itemize}
    \item Interactive AP placement optimization
    \item Real-time channel assignment
    \item Dynamic power control strategies
    \item Rapid what-if analysis for network modifications
\end{itemize}
\end{document}